{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Scikit-Learn\n",
    "\n",
    "In this notebook, we convert raw text into BERT embeddings.  This will allow us to perform natural language processing tasks such as text classification.\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Scikit-Learn are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Scikit-Learn in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  THIS NOTEBOOK WILL TAKE A 5-10 MINUTES TO COMPLETE.\n",
    "\n",
    "# PLEASE BE PATIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup Environment\n",
    "1. Setup Input Data\n",
    "1. Setup Output Data\n",
    "1. Build a Scikit-Learn container for running the processing job\n",
    "1. Run the Processing Job using Amazon SageMaker\n",
    "1. Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "s3 = boto3.Session().client(service_name=\"s3\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r s3_public_path_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://amazon-reviews-pds/tsv\n"
     ]
    }
   ],
   "source": [
    "print(s3_public_path_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r s3_private_path_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv\n"
     ]
    }
   ],
   "source": [
    "print(s3_private_path_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = \"s3://{}/amazon-reviews-pds/tsv/\".format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-12 14:57:24   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "2021-04-12 14:57:25   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\n",
      "2021-04-12 14:57:27   12134676 amazon_reviews_us_Gift_Card_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Processing Job using Amazon SageMaker\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job using our custom python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\n",
      "\u001b[37m# This is 2.3.0 (vs. 2.3.1 everywhere else) because we need to\u001b[39;49;00m\n",
      "\u001b[37m# use anaconda and anaconda only supports 2.3.0 at this time\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33manaconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow==2.3.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker==2.24.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msession\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Session\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_group\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FeatureGroup\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_store\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfeature_definition\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m (\n",
      "    FeatureDefinition,\n",
      "    FeatureTypeEnum,\n",
      ")\n",
      "\n",
      "region = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mAWS_DEFAULT_REGION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRegion: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(region))\n",
      "\n",
      "\u001b[37m#############################\u001b[39;49;00m\n",
      "\u001b[37m## We may need to get the Role and Bucket before setting sm, featurestore_runtime, etc.\u001b[39;49;00m\n",
      "\u001b[37m## Role and Bucket are malformed if we do this later.\u001b[39;49;00m\n",
      "sts = boto3.Session(region_name=region).client(service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "caller_identity = sts.get_caller_identity()\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcaller_identity: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(caller_identity))\n",
      "\n",
      "assumed_role_arn = caller_identity[\u001b[33m\"\u001b[39;49;00m\u001b[33mArn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m(assumed_role) caller_identity_arn: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(assumed_role_arn))\n",
      "\n",
      "assumed_role_name = assumed_role_arn.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[-\u001b[34m2\u001b[39;49;00m]\n",
      "\n",
      "iam = boto3.Session(region_name=region).client(service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33miam\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region)\n",
      "get_role_response = iam.get_role(RoleName=assumed_role_name)\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mget_role_response \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(get_role_response))\n",
      "role = get_role_response[\u001b[33m\"\u001b[39;49;00m\u001b[33mRole\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mArn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrole \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(role))\n",
      "\n",
      "bucket = sagemaker.Session().default_bucket()\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe DEFAULT BUCKET is \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(bucket))\n",
      "\u001b[37m#############################\u001b[39;49;00m\n",
      "\n",
      "sm = boto3.Session(region_name=region).client(service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "featurestore_runtime = boto3.Session(region_name=region).client(\n",
      "    service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msagemaker-featurestore-runtime\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region\n",
      ")\n",
      "\n",
      "s3 = boto3.Session(region_name=region).client(service_name=\u001b[33m\"\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, region_name=region)\n",
      "\n",
      "sagemaker_session = sagemaker.Session(\n",
      "    boto_session=boto3.Session(region_name=region),\n",
      "    sagemaker_client=sm,\n",
      "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
      ")\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "REVIEW_BODY_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "REVIEW_ID_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\n",
      "\n",
      "LABEL_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "label_map = {}\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\n",
      "    label_map[label] = i\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m\"\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mwait_for_feature_group_creation_complete\u001b[39;49;00m(feature_group):\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(status))\n",
      "        \u001b[34mwhile\u001b[39;49;00m status == \u001b[33m\"\u001b[39;49;00m\u001b[33mCreating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for Feature Group Creation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            time.sleep(\u001b[34m5\u001b[39;49;00m)\n",
      "            status = feature_group.describe().get(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroupStatus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(status))\n",
      "        \u001b[34mif\u001b[39;49;00m status != \u001b[33m\"\u001b[39;49;00m\u001b[33mCreated\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Group status: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(status))\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFailed to create feature group \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mFeatureGroup \u001b[39;49;00m\u001b[33m{feature_group.name}\u001b[39;49;00m\u001b[33m successfully created.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mNo feature group created yet.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_or_load_feature_group\u001b[39;49;00m(prefix, feature_group_name):\n",
      "\n",
      "    \u001b[37m# Feature Definitions for our records\u001b[39;49;00m\n",
      "    feature_definitions = [\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.INTEGRAL),\n",
      "        \u001b[37m#        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "        FeatureDefinition(feature_name=\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, feature_type=FeatureTypeEnum.STRING),\n",
      "    ]\n",
      "\n",
      "    feature_group = FeatureGroup(\n",
      "        name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sagemaker_session\n",
      "    )\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Group: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(feature_group))\n",
      "\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for existing Feature Group to become available if it is being created by another instance in our cluster...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        )\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBefore CREATE FG wait exeption: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(e))\n",
      "    \u001b[37m#        pass\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        record_identifier_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        event_time_feature_name = \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCreating Feature Group with role \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(role))\n",
      "        feature_group.create(\n",
      "            s3_uri=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{bucket}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{prefix}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            record_identifier_name=record_identifier_feature_name,\n",
      "            event_time_feature_name=event_time_feature_name,\n",
      "            role_arn=role,\n",
      "            enable_online_store=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        )\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCreating Feature Group. Completed.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for new Feature Group to become available...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        wait_for_feature_group_creation_complete(feature_group)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature Group available.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        feature_group.describe()\n",
      "\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m e:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mException: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(e))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m feature_group\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
      "        \u001b[37m#               review_body):\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\n",
      "\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m          text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\n",
      "\u001b[33m            sequence tasks, only this sequence must be specified.\u001b[39;49;00m\n",
      "\u001b[33m          label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\n",
      "\u001b[33m            specified for train and dev examples, but not for test examples.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.text = text\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    tokens = tokenizer.tokenize(the_input.text)\n",
      "\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    encode_plus_tokens = tokenizer.encode_plus(\n",
      "        the_input.text,\n",
      "        pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        max_length=max_seq_length,\n",
      "        \u001b[37m#                                               truncation=True\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\n",
      "\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\n",
      "    label_id = label_map[the_input.label]\n",
      "\n",
      "    features = InputFeatures(\n",
      "        input_ids=input_ids,\n",
      "        input_mask=input_mask,\n",
      "        segment_ids=segment_ids,\n",
      "        label_id=label_id,\n",
      "        review_id=the_input.review_id,\n",
      "        date=the_input.date,\n",
      "        label=the_input.label,\n",
      "    )\n",
      "    \u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\n",
      "    \u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs, output_file, max_seq_length):\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    records = []\n",
      "\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\n",
      "\n",
      "        features = convert_input(the_input, max_seq_length)\n",
      "\n",
      "        all_features = collections.OrderedDict()\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
      "\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\n",
      "\n",
      "        records.append(\n",
      "            {  \u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_mask,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.segment_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.review_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.date,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label,\n",
      "                \u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\n",
      "            }\n",
      "        )\n",
      "\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m# break\u001b[39;49;00m\n",
      "\n",
      "    tf_record_writer.close()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m records\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m  \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, max_seq_length, balance_dataset, prefix, feature_group_name):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(file))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(max_seq_length))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(balance_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(prefix))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(feature_group_name))\n",
      "\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\n",
      "    feature_group = create_or_load_feature_group(prefix, feature_group_name)\n",
      "\n",
      "    filename_without_extension = Path(Path(file).stem).stem\n",
      "\n",
      "    df = pd.read_csv(file, delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, quoting=csv.QUOTE_NONE, compression=\u001b[33m\"\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    df.isna().values.any()\n",
      "    df = df.dropna()\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\n",
      "        df_grouped_by = df.groupby([\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \n",
      "        df_balanced = df_grouped_by.apply(\u001b[34mlambda\u001b[39;49;00m x: x.sample(df_grouped_by.size().min()).reset_index(drop=\u001b[34mTrue\u001b[39;49;00m))\n",
      "\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_balanced.shape))\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\n",
      "\n",
      "        df = df_balanced\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.train_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.validation_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.test_split_percentage))\n",
      "\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(holdout_percentage))\n",
      "    \n",
      "    df_train, df_holdout = train_test_split(df, test_size=holdout_percentage, stratify=df[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_holdout_percentage))\n",
      "    \n",
      "    df_validation, df_test = train_test_split(\n",
      "        df_holdout, test_size=test_holdout_percentage, stratify=df_holdout[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_validation.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_test.shape))\n",
      "\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\n",
      "\n",
      "    train_inputs = df_train.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    validation_inputs = df_validation.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    test_inputs = df_test.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\n",
      "    train_records = transform_inputs_to_tfrecord(\n",
      "        train_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    validation_records = transform_inputs_to_tfrecord(\n",
      "        validation_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    test_records = transform_inputs_to_tfrecord(\n",
      "        test_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
      "    df_train_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_train_records.head()\n",
      "\n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
      "    df_validation_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_validation_records.head()\n",
      "\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\n",
      "    df_test_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_test_records.head()\n",
      "\n",
      "    \u001b[37m# Add record to feature store\u001b[39;49;00m\n",
      "    df_fs_train_records = cast_object_to_string(df_train_records)\n",
      "    df_fs_validation_records = cast_object_to_string(df_validation_records)\n",
      "    df_fs_test_records = cast_object_to_string(df_test_records)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mIngesting Features...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    feature_group.ingest(data_frame=df_fs_train_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    feature_group.ingest(data_frame=df_fs_validation_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    feature_group.ingest(data_frame=df_fs_test_records, max_workers=\u001b[34m3\u001b[39;49;00m, wait=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mFeature ingest completed.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.current_host))\n",
      "\n",
      "    feature_group = create_or_load_feature_group(\n",
      "        prefix=args.feature_store_offline_prefix, feature_group_name=args.feature_group_name\n",
      "    )\n",
      "\n",
      "    feature_group.describe()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(feature_group.as_hive_ddl())\n",
      "\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    transform_tsv_to_tfrecord = functools.partial(\n",
      "        _transform_tsv_to_tfrecord,\n",
      "        max_seq_length=args.max_seq_length,\n",
      "        balance_dataset=args.balance_dataset,\n",
      "        prefix=args.feature_store_offline_prefix,\n",
      "        feature_group_name=args.feature_group_name,\n",
      "    )\n",
      "\n",
      "    input_files = glob.glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data))\n",
      "\n",
      "    num_cpus = multiprocessing.cpu_count()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_cpus))\n",
      "\n",
      "    p = multiprocessing.Pool(num_cpus)\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data))\n",
      "    dirs_output = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data))\n",
      "    dirs_output = os.listdir(train_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data))\n",
      "    dirs_output = os.listdir(validation_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data))\n",
      "    dirs_output = os.listdir(test_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    offline_store_contents = \u001b[34mNone\u001b[39;49;00m\n",
      "    \u001b[34mwhile\u001b[39;49;00m offline_store_contents \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        objects_in_bucket = s3.list_objects(Bucket=bucket, Prefix=args.feature_store_offline_prefix)\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m objects_in_bucket \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(objects_in_bucket[\u001b[33m\"\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) > \u001b[34m1\u001b[39;49;00m:\n",
      "            offline_store_contents = objects_in_bucket[\u001b[33m\"\u001b[39;49;00m\u001b[33mContents\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWaiting for data in offline store...\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            sleep(\u001b[34m60\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mData available.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess-scikit-text-to-bert-feature-store.py` script.\n",
    "\n",
    "Note that we sharding the data using `ShardedByS3Key` to spread the transformations across all worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the `Experiment`\n",
    "We will track every step of this experiment throughout the `prepare`, `train`, `optimize`, and `deploy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts\n",
    "\n",
    "**Experiment**: A collection of related Trials.  Add Trials to an Experiment that you wish to compare together.\n",
    "\n",
    "**Trial**: A description of a multi-step machine learning workflow. Each step in the workflow is described by a Trial Component. There is no relationship between Trial Components such as ordering.\n",
    "\n",
    "**Trial Component**: A description of a single step in a machine learning workflow. For example data cleaning, feature extraction, model training, model evaluation, etc.\n",
    "\n",
    "**Tracker**: A logger of information about a single TrialComponent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: Amazon-Customer-Reviews-BERT-Experiment-1618247311\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "experiment = Experiment.create(\n",
    "    experiment_name=\"Amazon-Customer-Reviews-BERT-Experiment-{}\".format(timestamp),\n",
    "    description=\"Amazon Customer Reviews BERT Experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "experiment_name = experiment.experiment_name\n",
    "print(\"Experiment name: {}\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1618247314\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial = Trial.create(\n",
    "    trial_name=\"trial-{}\".format(timestamp), experiment_name=experiment_name, sagemaker_boto_client=sm\n",
    ")\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print(\"Trial name: {}\".format(trial_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"TrialName\": trial_name,\n",
    "    \"TrialComponentDisplayName\": \"prepare\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Customer-Reviews-BERT-Experiment-1618247311\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial-1618247314\n"
     ]
    }
   ],
   "source": [
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature Store and Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name=\"sagemaker-featurestore-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-1618247324\n"
     ]
    }
   ],
   "source": [
    "timestamp = int(time.time())\n",
    "\n",
    "feature_store_offline_prefix = \"reviews-feature-store-\" + str(timestamp)\n",
    "\n",
    "print(feature_store_offline_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-1618247324\n"
     ]
    }
   ],
   "source": [
    "feature_group_name = \"reviews-feature-group-\" + str(timestamp)\n",
    "\n",
    "print(feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name=\"input_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"input_mask\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"segment_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label_id\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"review_id\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"date\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    #    FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-1618247324', sagemaker_session=<sagemaker.session.Session object at 0x7fa7702c3b90>, feature_definitions=[FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>), FeatureDefinition(feature_name='review_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sess)\n",
    "\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Processing Job Hyper-Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processing_instance_type = \"ml.c5.2xlarge\"\n",
    "processing_instance_count = 2\n",
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05\n",
    "balance_dataset = True\n",
    "max_seq_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a `max_seq_length` for BERT\n",
    "Since a smaller `max_seq_length` leads to faster training and lower resource utilization, we want to find the smallest review length that captures `80%` of our reviews.\n",
    "\n",
    "Remember our distribution of review lengths from a previous section?\n",
    "\n",
    "```\n",
    "mean         51.683405\n",
    "std         107.030844\n",
    "min           1.000000\n",
    "10%           2.000000\n",
    "20%           7.000000\n",
    "30%          19.000000\n",
    "40%          22.000000\n",
    "50%          26.000000\n",
    "60%          32.000000\n",
    "70%          43.000000\n",
    "80%          63.000000\n",
    "90%         110.000000\n",
    "100%       5347.000000\n",
    "max        5347.000000\n",
    "```\n",
    "\n",
    "\n",
    "Review length `63` represents the `80th` percentile for this dataset.  However, it's best to stick with powers-of-2 when using BERT.  So let's choose `64` as this is the smallest power-of-2 greater than `63`.  Reviews with length > `64` will be truncated to `64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    "    max_runtime_in_seconds=7200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2021-04-12-17-09-10-985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2021-04-12-17-09-10-985\n",
      "Inputs:  [{'InputName': 'raw-input-data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor.run(\n",
    "    code=\"preprocess-scikit-text-to-bert-feature-store.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"raw-input-data\",\n",
    "            source=raw_input_data_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/data/\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-train\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/bert/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-validation\",\n",
    "            s3_upload_mode=\"EndOfJob\",\n",
    "            source=\"/opt/ml/processing/output/bert/validation\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-test\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/bert/test\"\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--train-split-percentage\",\n",
    "        str(train_split_percentage),\n",
    "        \"--validation-split-percentage\",\n",
    "        str(validation_split_percentage),\n",
    "        \"--test-split-percentage\",\n",
    "        str(test_split_percentage),\n",
    "        \"--max-seq-length\",\n",
    "        str(max_seq_length),\n",
    "        \"--balance-dataset\",\n",
    "        str(balance_dataset),\n",
    "        \"--feature-store-offline-prefix\",\n",
    "        str(feature_store_offline_prefix),\n",
    "        \"--feature-group-name\",\n",
    "        str(feature_group_name),\n",
    "    ],\n",
    "    experiment_config=experiment_config,\n",
    "    logs=True,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2021-04-12-17-09-10-985\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()[\"ProcessingJobName\"]\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2021-04-12-17-09-10-985\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(\n",
    "            region, scikit_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2021-04-12-17-09-10-985;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\n",
    "            region, scikit_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(\n",
    "            bucket, scikit_processing_job_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'raw-input-data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2021-04-12-17-09-10-985', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '64', '--balance-dataset', 'True', '--feature-store-offline-prefix', 'reviews-feature-store-1618247324', '--feature-group-name', 'reviews-feature-group-1618247324']}, 'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'}, 'RoleArn': 'arn:aws:iam::117859797117:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole', 'ExperimentConfig': {'ExperimentName': 'Amazon-Customer-Reviews-BERT-Experiment-1618247311', 'TrialName': 'trial-1618247314', 'TrialComponentDisplayName': 'prepare'}, 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2021, 4, 12, 17, 9, 12, 120000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2021, 4, 12, 17, 9, 11, 481000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'cce3e55b-b3e1-4914-ab0e-3a2bfd6b981a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'cce3e55b-b3e1-4914-ab0e-3a2bfd6b981a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2978', 'date': 'Mon, 12 Apr 2021 17:09:19 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=scikit_processing_job_name, sagemaker_session=sess\n",
    ")\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Data\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train\n",
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation\n",
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "output_config = processing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"bert-train\":\n",
    "        processed_train_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bert-validation\":\n",
    "        processed_validation_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bert-test\":\n",
    "        processed_test_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:19:49   10478979 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:49    2313103 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:24   11706723 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:19:50     582980 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:50     128566 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:25     651334 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-12 17:19:50     581539 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:50     128821 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:25     651069 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'raw_input_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store train_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store validation_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store test_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'balance_dataset' (bool)\n"
     ]
    }
   ],
   "source": [
    "%store balance_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'feature_store_offline_prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store feature_store_offline_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'feature_group_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store feature_group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_train_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_validation_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "balance_dataset                                       -> True\n",
      "balanced_bias_data_jsonlines_s3_uri                   -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "balanced_bias_data_s3_uri                             -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "bias_data_s3_uri                                      -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "experiment_name                                       -> 'Amazon-Customer-Reviews-BERT-Experiment-161824731\n",
      "feature_group_name                                    -> 'reviews-feature-group-1618247324'\n",
      "feature_store_offline_prefix                          -> 'reviews-feature-store-1618247324'\n",
      "ingest_create_athena_db_passed                        -> True\n",
      "ingest_create_athena_table_parquet_passed             -> True\n",
      "ingest_create_athena_table_tsv_passed                 -> True\n",
      "max_seq_length                                        -> 64\n",
      "model_ab_endpoint_name                                -> 'tensorflow-training-2021-04-05-11-23-57-968-abtes\n",
      "processed_metrics_s3_uri                              -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_test_data_s3_uri                            -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_train_data_s3_uri                           -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_validation_data_s3_uri                      -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processing_evaluation_metrics_job_name                -> 'sagemaker-scikit-learn-2021-04-05-13-42-30-315'\n",
      "raw_input_data_s3_uri                                 -> 's3://sagemaker-us-east-1-117859797117/amazon-revi\n",
      "s3_private_path_tsv                                   -> 's3://sagemaker-us-east-1-117859797117/amazon-revi\n",
      "s3_public_path_tsv                                    -> 's3://amazon-reviews-pds/tsv'\n",
      "setup_dependencies_passed                             -> True\n",
      "tensorflow_endpoint_arn                               -> 'arn:aws:sagemaker:us-east-1:117859797117:endpoint\n",
      "tensorflow_endpoint_name                              -> 'tensorflow-training-2021-04-05-11-23-57-968-tf-16\n",
      "tensorflow_model_name                                 -> 'tensorflow-training-2021-04-05-11-23-57-968-tf-16\n",
      "test_data_bias_s3_uri                                 -> 's3://sagemaker-us-east-1-117859797117/bias/test_d\n",
      "test_data_explainablity_s3_uri                        -> 's3://sagemaker-us-east-1-117859797117/bias/test_d\n",
      "test_split_percentage                                 -> 0.05\n",
      "train_split_percentage                                -> 0.9\n",
      "training_job_name                                     -> 'tensorflow-training-2021-04-05-11-23-57-968'\n",
      "trial_name                                            -> 'trial-1618247314'\n",
      "validation_split_percentage                           -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query The Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_query = feature_group.athena_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_table = feature_store_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running \n",
      "SELECT input_ids, input_mask, segment_ids, label_id, split_type  FROM \"reviews-feature-group-1618247324-1618247746\" WHERE split_type='train' LIMIT 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT input_ids, input_mask, segment_ids, label_id, split_type  FROM \"{}\" WHERE split_type='train' LIMIT 5\n",
    "\"\"\".format(\n",
    "    feature_store_table\n",
    ")\n",
    "\n",
    "print(\"Running \" + query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query f09405ea-74f6-44c4-94ec-17139c0642e9 is being executed.\n",
      "INFO:sagemaker:Query f09405ea-74f6-44c4-94ec-17139c0642e9 successfully executed.\n"
     ]
    }
   ],
   "source": [
    "feature_store_query.run(\n",
    "    query_string=query_string,\n",
    "    output_location=\"s3://\" + bucket + \"/\" + feature_store_offline_prefix + \"/query_results/\",\n",
    ")\n",
    "\n",
    "feature_store_query.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>input_mask</th>\n",
       "      <th>segment_ids</th>\n",
       "      <th>label_id</th>\n",
       "      <th>split_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 1045, 2985, 1002, 2322, 6363, 2006, 2023...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[101, 2037, 8860, 2024, 2200, 3132, 1998, 2130...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[101, 1045, 2293, 1996, 2208, 2021, 1045, 2018...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[101, 8816, 2075, 1996, 4007, 2001, 2025, 2004...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[101, 1045, 2052, 2025, 16755, 2023, 4171, 400...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           input_ids  \\\n",
       "0  [101, 1045, 2985, 1002, 2322, 6363, 2006, 2023...   \n",
       "1  [101, 2037, 8860, 2024, 2200, 3132, 1998, 2130...   \n",
       "2  [101, 1045, 2293, 1996, 2208, 2021, 1045, 2018...   \n",
       "3  [101, 8816, 2075, 1996, 4007, 2001, 2025, 2004...   \n",
       "4  [101, 1045, 2052, 2025, 16755, 2023, 4171, 400...   \n",
       "\n",
       "                                          input_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                         segment_ids  label_id split_type  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         0      train  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         1      train  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         2      train  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         1      train  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         1      train  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store_query.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>AWS_DEFAULT_REGION</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>SageMaker.ImageUri - MediaType</th>\n",
       "      <th>SageMaker.ImageUri - Value</th>\n",
       "      <th>code - MediaType</th>\n",
       "      <th>...</th>\n",
       "      <th>raw-input-data - MediaType</th>\n",
       "      <th>raw-input-data - Value</th>\n",
       "      <th>bert-test - MediaType</th>\n",
       "      <th>bert-test - Value</th>\n",
       "      <th>bert-train - MediaType</th>\n",
       "      <th>bert-train - Value</th>\n",
       "      <th>bert-validation - MediaType</th>\n",
       "      <th>bert-validation - Value</th>\n",
       "      <th>Trials</th>\n",
       "      <th>Experiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job</td>\n",
       "      <td>prepare</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train</td>\n",
       "      <td>None</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation</td>\n",
       "      <td>[trial-1618247314]</td>\n",
       "      <td>[Amazon-Customer-Reviews-BERT-Experiment-1618247311]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TrialComponentName  \\\n",
       "0  sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job   \n",
       "\n",
       "  DisplayName  \\\n",
       "0     prepare   \n",
       "\n",
       "                                                                                                SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985   \n",
       "\n",
       "  AWS_DEFAULT_REGION  SageMaker.InstanceCount SageMaker.InstanceType  \\\n",
       "0          us-east-1                      2.0          ml.c5.2xlarge   \n",
       "\n",
       "   SageMaker.VolumeSizeInGB SageMaker.ImageUri - MediaType  \\\n",
       "0                      30.0                           None   \n",
       "\n",
       "                                                           SageMaker.ImageUri - Value  \\\n",
       "0  683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3   \n",
       "\n",
       "  code - MediaType  ... raw-input-data - MediaType  \\\n",
       "0             None  ...                       None   \n",
       "\n",
       "                                          raw-input-data - Value  \\\n",
       "0  s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/   \n",
       "\n",
       "  bert-test - MediaType  \\\n",
       "0                  None   \n",
       "\n",
       "                                                                                       bert-test - Value  \\\n",
       "0  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test   \n",
       "\n",
       "  bert-train - MediaType  \\\n",
       "0                   None   \n",
       "\n",
       "                                                                                       bert-train - Value  \\\n",
       "0  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train   \n",
       "\n",
       "  bert-validation - MediaType  \\\n",
       "0                        None   \n",
       "\n",
       "                                                                                       bert-validation - Value  \\\n",
       "0  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation   \n",
       "\n",
       "               Trials                                           Experiments  \n",
       "0  [trial-1618247314]  [Amazon-Customer-Reviews-BERT-Experiment-1618247311]  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "# pd.set_option(\"max_rows\", 100)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, experiment_name=experiment_name, sort_by=\"CreationTime\", sort_order=\"Descending\"\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job\n"
     ]
    }
   ],
   "source": [
    "trial_component_name = experiment_analytics_df.TrialComponentName[0]\n",
    "print(trial_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TrialComponentName': 'sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job',\n",
       " 'TrialComponentArn': 'arn:aws:sagemaker:us-east-1:117859797117:experiment-trial-component/sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job',\n",
       " 'DisplayName': 'prepare',\n",
       " 'Source': {'SourceArn': 'arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985',\n",
       "  'SourceType': 'SageMakerProcessingJob'},\n",
       " 'Status': {'PrimaryStatus': 'Completed',\n",
       "  'Message': 'Status: Completed, exit message: null, failure reason: null'},\n",
       " 'StartTime': datetime.datetime(2021, 4, 12, 17, 13, 13, tzinfo=tzlocal()),\n",
       " 'EndTime': datetime.datetime(2021, 4, 12, 17, 20, 30, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 4, 12, 17, 9, 11, 872000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:117859797117:user-profile/d-kbozpoygmni6/tuhin-sagemaker-studio',\n",
       "  'UserProfileName': 'tuhin-sagemaker-studio',\n",
       "  'DomainId': 'd-kbozpoygmni6'},\n",
       " 'LastModifiedTime': datetime.datetime(2021, 4, 12, 17, 20, 31, 115000, tzinfo=tzlocal()),\n",
       " 'LastModifiedBy': {},\n",
       " 'Parameters': {'AWS_DEFAULT_REGION': {'StringValue': 'us-east-1'},\n",
       "  'SageMaker.InstanceCount': {'NumberValue': 2.0},\n",
       "  'SageMaker.InstanceType': {'StringValue': 'ml.c5.2xlarge'},\n",
       "  'SageMaker.VolumeSizeInGB': {'NumberValue': 30.0}},\n",
       " 'InputArtifacts': {'SageMaker.ImageUri': {'Value': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
       "  'code': {'Value': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/input/code/preprocess-scikit-text-to-bert-feature-store.py'},\n",
       "  'raw-input-data': {'Value': 's3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/'}},\n",
       " 'OutputArtifacts': {'bert-test': {'Value': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test'},\n",
       "  'bert-train': {'Value': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train'},\n",
       "  'bert-validation': {'Value': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation'}},\n",
       " 'Metrics': [],\n",
       " 'ResponseMetadata': {'RequestId': '8fccb862-8f0d-4bb5-8d46-95f3c07c537d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8fccb862-8f0d-4bb5-8d46-95f3c07c537d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1909',\n",
       "   'date': 'Mon, 12 Apr 2021 17:20:43 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_component_description = sm.describe_trial_component(TrialComponentName=trial_component_name)\n",
    "trial_component_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show SageMaker ML Lineage Tracking \n",
    "\n",
    "Amazon SageMaker ML Lineage Tracking creates and stores information about the steps of a machine learning (ML) workflow from data preparation to model deployment. \n",
    "\n",
    "Amazon SageMaker Lineage enables events that happen within SageMaker to be traced via a graph structure. The data simplifies generating reports, making comparisons, or discovering relationships between events. For example easily trace both how a model was generated and where the model was deployed.\n",
    "\n",
    "The lineage graph is created automatically by SageMaker and you can directly create or modify your own graphs.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "* **Lineage Graph** - A connected graph tracing your machine learning workflow end to end.\n",
    "\n",
    "* **Artifacts** - Represents a URI addressable object or data. Artifacts are typically inputs or outputs to Actions.\n",
    "\n",
    "* **Actions** - Represents an action taken such as a computation, transformation, or job.\n",
    "\n",
    "* **Contexts** - Provides a method to logically group other entities.\n",
    "\n",
    "* **Associations** - A directed edge in the lineage graph that links two entities.\n",
    "\n",
    "* **Lineage Traversal** - Starting from an arbitrary point trace the lineage graph to discover and analyze relationships between steps in your workflow.\n",
    "\n",
    "* **Experiments** - Experiment entites (Experiments, Trials, and Trial Components) are also part of the lineage graph and can be associated wtih Artifacts, Actions, or Contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Lineage Artifacts For Our Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...ess-scikit-text-to-bert-feature-store.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...t-1-117859797117/amazon-reviews-pds/tsv/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...2021-04-12-17-09-10-985/output/bert-test</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...4-12-17-09-10-985/output/bert-validation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s3://...021-04-12-17-09-10-985/output/bert-train</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...ess-scikit-text-to-bert-feature-store.py     Input  DataSet   \n",
       "1  s3://...t-1-117859797117/amazon-reviews-pds/tsv/     Input  DataSet   \n",
       "2  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "3  s3://...2021-04-12-17-09-10-985/output/bert-test    Output  DataSet   \n",
       "4  s3://...4-12-17-09-10-985/output/bert-validation    Output  DataSet   \n",
       "5  s3://...021-04-12-17-09-10-985/output/bert-train    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  \n",
       "4         Produced     artifact  \n",
       "5         Produced     artifact  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "lineage_table_viz = LineageTableVisualizer(sess)\n",
    "lineage_table_viz_df = lineage_table_viz.show(processing_job_name=scikit_processing_job_name)\n",
    "lineage_table_viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
