{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  THIS NOTEBOOK WILL TAKE ABOUT 30 MINUTES TO COMPLETE.\n",
    "\n",
    "# PLEASE BE PATIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a BERT Model and Create a Text Classifier\n",
    "\n",
    "In the previous section, we've already performed the Feature Engineering to create BERT embeddings from the `reviews_body` text using the pre-trained BERT model, and split the dataset into train, validation and test files. To optimize for Tensorflow training, we saved the files in TFRecord format. \n",
    "\n",
    "Now, let’s fine-tune the BERT model to our Customer Reviews Dataset and add a new classification layer to predict the `star_rating` for a given `review_body`.\n",
    "\n",
    "\n",
    "As mentioned earlier, BERT’s attention mechanism is called a Transformer. This is, not coincidentally, the name of the popular BERT Python library, “Transformers,” maintained by a company called HuggingFace. We will use a variant of BERT called [DistilBert](https://arxiv.org/pdf/1910.01108.pdf) which requires less memory and compute, but maintains very good accuracy on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _PRE-REQUISITE: You need to have succesfully run the notebooks in the `PREPARE` section before you continue with this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation\n"
     ]
    }
   ],
   "source": [
    "print(processed_validation_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-Customer-Reviews-BERT-Experiment-1618247311\n"
     ]
    }
   ],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial-1618247314\n"
     ]
    }
   ],
   "source": [
    "print(trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Dataset in S3\n",
    "We are using the train, validation, and test splits created in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train\n",
      "2021-04-12 17:19:49   10478979 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:49    2313103 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:24   11706723 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_train_data_s3_uri)\n",
    "\n",
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation\n",
      "2021-04-12 17:19:50     582980 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:50     128566 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:25     651334 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_validation_data_s3_uri)\n",
    "\n",
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test\n",
      "2021-04-12 17:19:50     581539 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2021-04-12 17:19:50     128821 part-algo-1-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2021-04-12 17:20:25     651069 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(processed_test_data_s3_uri)\n",
    "\n",
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify S3 `Distribution Strategy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "s3_input_train_data = TrainingInput(s3_data=processed_train_data_s3_uri, distribution=\"ShardedByS3Key\")\n",
    "s3_input_validation_data = TrainingInput(s3_data=processed_validation_data_s3_uri, distribution=\"ShardedByS3Key\")\n",
    "s3_input_test_data = TrainingInput(s3_data=processed_test_data_s3_uri, distribution=\"ShardedByS3Key\")\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters for Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 0.00001\n",
    "epsilon = 0.00000001\n",
    "train_batch_size = 128\n",
    "validation_batch_size = 128\n",
    "test_batch_size = 128\n",
    "train_steps_per_epoch = 100\n",
    "validation_steps = 100\n",
    "test_steps = 100\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.c5.9xlarge\"\n",
    "train_volume_size = 1024\n",
    "use_xla = True\n",
    "use_amp = True\n",
    "freeze_bert_layer = False\n",
    "enable_sagemaker_debugger = True\n",
    "enable_checkpointing = False\n",
    "enable_tensorboard = True\n",
    "input_mode = \"File\"\n",
    "run_validation = True\n",
    "run_test = True\n",
    "run_sample_predictions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance\n",
    "\n",
    "These sample log lines...\n",
    "```\n",
    "45/50 [=====>..] - ETA: 3s - loss: 0.425 - accuracy: 0.881\n",
    "50/50 [=======>] - ETA: 0s - val_loss: 0.407 - val_accuracy: 0.885\n",
    "```\n",
    "...will produce the following 4 metrics in CloudWatch:\n",
    "\n",
    "`loss` = 0.425\n",
    "\n",
    "`accuracy` = 0.881\n",
    "\n",
    "`val_loss` = 0.407\n",
    "\n",
    "`val_accuracy` = 0.885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation:loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation:accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SageMaker Debugger\n",
    "Define Debugger Rules as deccribed here:  https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule\n",
    "from sagemaker.debugger import rule_configs\n",
    "from sagemaker.debugger import ProfilerRule\n",
    "from sagemaker.debugger import CollectionConfig\n",
    "from sagemaker.debugger import DebuggerHookConfig\n",
    "\n",
    "actions = rule_configs.ActionList(\n",
    "    #    rule_configs.StopTraining(),\n",
    "    #    rule_configs.Email(\"\")\n",
    ")\n",
    "\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),    \n",
    "#     ProfilerRule.sagemaker(rule_configs.BatchSize()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.CPUBottleneck()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.GPUMemoryIncrease()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.IOBottleneck()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.LoadBalancing()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.OverallSystemUsage()),\n",
    "#     ProfilerRule.sagemaker(rule_configs.StepOutlier()),\n",
    "#     Rule.sagemaker(\n",
    "#         base_config=rule_configs.loss_not_decreasing(),\n",
    "#         rule_parameters={\n",
    "#             \"collection_names\": \"losses,metrics\",\n",
    "#             \"use_losses_collection\": \"true\",\n",
    "#             \"num_steps\": \"10\",\n",
    "#             \"diff_percent\": \"50\",\n",
    "#         },\n",
    "#         collections_to_save=[\n",
    "#             CollectionConfig(\n",
    "#                 name=\"losses\",\n",
    "#                 parameters={\n",
    "#                     \"save_interval\": \"10\",\n",
    "#                 },\n",
    "#             ),\n",
    "#             CollectionConfig(\n",
    "#                 name=\"metrics\",\n",
    "#                 parameters={\n",
    "#                     \"save_interval\": \"10\",\n",
    "#                 },\n",
    "#             ),\n",
    "#         ],\n",
    "#         actions=actions,\n",
    "#     ),\n",
    "#     Rule.sagemaker(\n",
    "#         base_config=rule_configs.overtraining(),\n",
    "#         rule_parameters={\n",
    "#             \"collection_names\": \"losses,metrics\",\n",
    "#             \"patience_train\": \"10\",\n",
    "#             \"patience_validation\": \"10\",\n",
    "#             \"delta\": \"0.5\",\n",
    "#         },\n",
    "#         collections_to_save=[\n",
    "#             CollectionConfig(\n",
    "#                 name=\"losses\",\n",
    "#                 parameters={\n",
    "#                     \"save_interval\": \"10\",\n",
    "#                 },\n",
    "#             ),\n",
    "#             CollectionConfig(\n",
    "#                 name=\"metrics\",\n",
    "#                 parameters={\n",
    "#                     \"save_interval\": \"10\",\n",
    "#                 },\n",
    "#             ),\n",
    "#         ],\n",
    "#         actions=actions,\n",
    "#     )    \n",
    "]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\n",
    "        \"save_interval\": \"10\",  # number of steps\n",
    "        \"export_tensorboard\": \"true\",\n",
    "        \"tensorboard_dir\": \"hook_tensorboard/\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify a Debugger profiler configuration\n",
    "\n",
    "The following configuration will capture system metrics at 500 milliseconds. The system metrics include utilization per CPU, GPU, memory utilization per CPU, GPU as well I/O and network.\n",
    "\n",
    "Debugger will capture detailed profiling information from step 5 to step 15. This information includes Horovod metrics, dataloading, preprocessing, operators running on CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(local_path=\"/opt/ml/output/profiler/\", start_step=5, num_steps=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Checkpoint S3 Location\n",
    "This is used for Spot Instances Training.  If nodes are replaced, the new node will start training from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-117859797117/checkpoints/fab42251-467c-46a9-bfc6-8fa60a8276b1/\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "checkpoint_s3_prefix = \"checkpoints/{}\".format(str(uuid.uuid4()))\n",
    "checkpoint_s3_uri = \"s3://{}/{}/\".format(bucket, checkpoint_s3_prefix)\n",
    "\n",
    "print(checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Our BERT + TensorFlow Script to Run on SageMaker\n",
    "Prepare our TensorFlow model to run on the managed SageMaker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\u001b[39;49;00m\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\n",
      "    x = {\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]}\n",
      "\n",
      "    y = record[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(\n",
      "    channel,\n",
      "    input_filenames,\n",
      "    pipe_mode,\n",
      "    is_training,\n",
      "    drop_remainder,\n",
      "    batch_size,\n",
      "    epochs,\n",
      "    steps_per_epoch,\n",
      "    max_seq_length,\n",
      "):\n",
      "\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(channel))\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\n",
      "\n",
      "        dataset = PipeModeDataset(channel=channel, record_format=\u001b[33m\"\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_filenames))\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
      "\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\n",
      "    \u001b[37m#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49;00m\n",
      "\n",
      "    name_to_features = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\n",
      "\n",
      "    dataset = dataset.apply(\n",
      "        tf.data.experimental.map_and_batch(\n",
      "            \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\n",
      "            batch_size=batch_size,\n",
      "            drop_remainder=drop_remainder,\n",
      "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
      "        )\n",
      "    )\n",
      "\n",
      "    \u001b[37m#    dataset.cache()\u001b[39;49;00m\n",
      "\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m, reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(channel))\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m5\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint_model\u001b[39;49;00m(checkpoint_path):\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "    glob_pattern = os.path.join(checkpoint_path, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mglob pattern \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(glob_pattern))\n",
      "\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mList of checkpoint files \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(list_of_checkpoint_files))\n",
      "\n",
      "    latest_checkpoint_file = \u001b[36mmax\u001b[39;49;00m(list_of_checkpoint_files)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLatest checkpoint file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(latest_checkpoint_file))\n",
      "\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit(\u001b[33m\"\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m].split(\u001b[33m\"\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "    initial_epoch_number = \u001b[36mint\u001b[39;49;00m(initial_epoch_number_str)\n",
      "\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(latest_checkpoint_file, config=config)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mloaded_model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(loaded_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minitial_epoch_number \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(initial_epoch_number))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaded_model, initial_epoch_number\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--checkpoint_base_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00003\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])  \u001b[37m# This is unused\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument('--model_dir',\u001b[39;49;00m\n",
      "    \u001b[37m#                     type=str,\u001b[39;49;00m\n",
      "    \u001b[37m#                     default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    env_var = os.environ\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(env_var[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    sm_training_env_json = json.loads(env_var[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    is_master = sm_training_env_json[\u001b[33m\"\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mis_master \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_master))\n",
      "\n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data))\n",
      "    validation_data = args.validation_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data))\n",
      "    test_data = args.test_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data))\n",
      "    local_model_dir = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(output_dir))\n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(hosts))\n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(current_host))\n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_gpus))\n",
      "    job_name = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mjob_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(job_name))\n",
      "    use_xla = args.use_xla\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_xla))\n",
      "    use_amp = args.use_amp\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_amp))\n",
      "    max_seq_length = args.max_seq_length\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(max_seq_length))\n",
      "    train_batch_size = args.train_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_batch_size))\n",
      "    validation_batch_size = args.validation_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_batch_size))\n",
      "    test_batch_size = args.test_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_batch_size))\n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epochs))\n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(learning_rate))\n",
      "    epsilon = args.epsilon\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epsilon))\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_steps_per_epoch))\n",
      "    validation_steps = args.validation_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_steps))\n",
      "    test_steps = args.test_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_steps))\n",
      "    freeze_bert_layer = args.freeze_bert_layer\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(freeze_bert_layer))\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "    run_validation = args.run_validation\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_validation))\n",
      "    run_test = args.run_test\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_test))\n",
      "    run_sample_predictions = args.run_sample_predictions\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_sample_predictions))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_tensorboard))\n",
      "    enable_checkpointing = args.enable_checkpointing\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_checkpointing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_checkpointing))\n",
      "\n",
      "    checkpoint_base_path = args.checkpoint_base_path\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcheckpoint_base_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_base_path))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\n",
      "        checkpoint_path = checkpoint_base_path\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        checkpoint_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcheckpoint_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_path))\n",
      "\n",
      "    \u001b[37m# Determine if PipeMode is enabled\u001b[39;49;00m\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    pipe_mode = pipe_mode_str.find(\u001b[33m\"\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(pipe_mode))\n",
      "\n",
      "    \u001b[37m# Model Output\u001b[39;49;00m\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Tensorboard Logs\u001b[39;49;00m\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Commented out due to incompatibility with transformers library (possibly)\u001b[39;49;00m\n",
      "    \u001b[37m# Set the global precision mixed_precision policy to \"mixed_float16\"\u001b[39;49;00m\n",
      "    \u001b[37m#    mixed_precision_policy = 'mixed_float16'\u001b[39;49;00m\n",
      "    \u001b[37m#    print('Mixed precision policy {}'.format(mixed_precision_policy))\u001b[39;49;00m\n",
      "    \u001b[37m#    policy = mixed_precision.Policy(mixed_precision_policy)\u001b[39;49;00m\n",
      "    \u001b[37m#    mixed_precision.set_policy(policy)\u001b[39;49;00m\n",
      "\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
      "    \u001b[37m# Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\u001b[39;49;00m\n",
      "    \u001b[37m# distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\n",
      "        tf.config.optimizer.set_jit(use_xla)\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\n",
      "\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data_filenames))\n",
      "        train_dataset = file_based_input_dataset_builder(\n",
      "            channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            input_filenames=train_data_filenames,\n",
      "            pipe_mode=pipe_mode,\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            batch_size=train_batch_size,\n",
      "            epochs=epochs,\n",
      "            steps_per_epoch=train_steps_per_epoch,\n",
      "            max_seq_length=max_seq_length,\n",
      "        ).map(select_data_and_label_from_record)\n",
      "\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\n",
      "        transformer_model = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download:\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                config = DistilBertConfig.from_pretrained(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                    num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "                    id2label={\u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m},\n",
      "                    label2id={\u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m},\n",
      "                )\n",
      "\n",
      "                transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config=config\n",
      "                )\n",
      "\n",
      "                input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "                embedding_layer = transformer_model.distilbert(input_ids, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "                X = tf.keras.layers.Bidirectional(\n",
      "                    tf.keras.layers.LSTM(\u001b[34m50\u001b[39;49;00m, return_sequences=\u001b[34mTrue\u001b[39;49;00m, dropout=\u001b[34m0.1\u001b[39;49;00m, recurrent_dropout=\u001b[34m0.1\u001b[39;49;00m)\n",
      "                )(embedding_layer)\n",
      "                X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[34m50\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[36mlen\u001b[39;49;00m(CLASSES), activation=\u001b[33m\"\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(X)\n",
      "\n",
      "                model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=X)\n",
      "\n",
      "                \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "                    layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(retries))\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(retries, random_sleep))\n",
      "                time.sleep(random_sleep)\n",
      "\n",
      "        callbacks = []\n",
      "\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_checkpointing:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Checkpoint enabled *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "            os.makedirs(checkpoint_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            \u001b[34mif\u001b[39;49;00m os.listdir(checkpoint_path):\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Found checkpoint *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(checkpoint_path)\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using checkpoint model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\n",
      "\n",
      "            checkpoint_callback = ModelCheckpoint(\n",
      "                filepath=os.path.join(checkpoint_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mtf_model_\u001b[39;49;00m\u001b[33m{epoch:05d}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "                save_weights_only=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                verbose=\u001b[34m1\u001b[39;49;00m,\n",
      "                monitor=\u001b[33m\"\u001b[39;49;00m\u001b[33mval_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** CHECKPOINT CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_callback))\n",
      "            callbacks.append(checkpoint_callback)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_amp))\n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m\"\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\n",
      "\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** DEBUGGER CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(debugger_callback))\n",
      "            callbacks.append(debugger_callback)\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_tensorboard:\n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** TENSORBOARD CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(tensorboard_callback))\n",
      "            callbacks.append(tensorboard_callback)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(optimizer))\n",
      "\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\n",
      "        \u001b[37m#        model.layers[0].trainable = not freeze_bert_layer\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data_filenames))\n",
      "            validation_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                input_filenames=validation_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=validation_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=validation_steps,\n",
      "                max_seq_length=max_seq_length,\n",
      "            ).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\n",
      "            train_and_validation_history = model.fit(\n",
      "                train_dataset,\n",
      "                shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                epochs=epochs,\n",
      "                initial_epoch=initial_epoch_number,\n",
      "                steps_per_epoch=train_steps_per_epoch,\n",
      "                validation_data=validation_dataset,\n",
      "                validation_steps=validation_steps,\n",
      "                callbacks=callbacks,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\n",
      "        \u001b[34melse\u001b[39;49;00m:  \u001b[37m# Not running validation\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            train_history = model.fit(\n",
      "                train_dataset,\n",
      "                shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                epochs=epochs,\n",
      "                initial_epoch=initial_epoch_number,\n",
      "                steps_per_epoch=train_steps_per_epoch,\n",
      "                callbacks=callbacks,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data_filenames))\n",
      "            test_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                input_filenames=test_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=test_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=test_steps,\n",
      "                max_seq_length=max_seq_length,\n",
      "            ).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            test_history = model.evaluate(test_dataset, steps=test_steps, callbacks=callbacks)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_history))\n",
      "\n",
      "        \u001b[37m# Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(transformer_fine_tuned_model_path))\n",
      "        transformer_model.save_pretrained(transformer_fine_tuned_model_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel inputs after save_pretrained: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model.inputs))\n",
      "\n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(tensorflow_saved_model_path))\n",
      "        model.save(tensorflow_saved_model_path, include_optimizer=\u001b[34mFalse\u001b[39;49;00m, overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "        \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "        \u001b[37m#         This appears to be hard-coded and must be called code/\u001b[39;49;00m\n",
      "        inference_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCopying inference source files to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(inference_path))\n",
      "        os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        os.system(\u001b[33m\"\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(inference_path))\n",
      "        \u001b[36mprint\u001b[39;49;00m(glob(inference_path))\n",
      "        \u001b[37m#        os.system('cp requirements.txt {}/code'.format(inference_path))\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# Copy test data for the evaluation step\u001b[39;49;00m\n",
      "        os.system(\u001b[33m\"\u001b[39;49;00m\u001b[33mcp -R ./test_data/ \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(local_model_dir))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\n",
      "            encode_plus_tokens = tokenizer.encode_plus(\n",
      "                text, pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m, max_length=max_seq_length, truncation=\u001b[34mTrue\u001b[39;49;00m, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            )\n",
      "            \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "            input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "            \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "            input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "            outputs = model.predict(x=(input_ids, input_mask))\n",
      "\n",
      "            prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m outputs]\n",
      "\n",
      "            \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\"\"I loved it!  I will recommend this to everyone.\"\"\"\u001b[39;49;00m,\n",
      "            predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\n",
      "        )\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\u001b[39;49;00m,\n",
      "            predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\n",
      "        )\n",
      "\n",
      "        df_test_reviews = pd.read_csv(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            quoting=csv.QUOTE_NONE,\n",
      "            compression=\u001b[33m\"\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        )[[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\n",
      "\n",
      "        df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "        df_test_reviews.shape\n",
      "        df_test_reviews.head()\n",
      "\n",
      "        y_test = df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].map(predict)\n",
      "        y_test\n",
      "\n",
      "        y_actual = df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        y_actual\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\n",
      "        accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, accuracy)\n",
      "\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap=plt.cm.Greens):\n",
      "            \u001b[36mprint\u001b[39;49;00m(cm)\n",
      "            plt.imshow(cm, interpolation=\u001b[33m\"\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, cmap=cmap)\n",
      "            plt.title(title)\n",
      "            plt.colorbar()\n",
      "            tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\n",
      "            plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\n",
      "            plt.yticks(tick_marks, classes)\n",
      "\n",
      "            fmt = \u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            thresh = cm.max() / \u001b[34m2.0\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\n",
      "                plt.text(\n",
      "                    j,\n",
      "                    i,\n",
      "                    \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\n",
      "                    horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                    color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                )\n",
      "\n",
      "                plt.tight_layout()\n",
      "                plt.ylabel(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                plt.xlabel(\u001b[33m\"\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
      "\n",
      "        plt.figure()\n",
      "        fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m))\n",
      "        plot_conf_mat(cm, classes=[\u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], title=\u001b[33m\"\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save the confusion matrix\u001b[39;49;00m\n",
      "        plt.show()\n",
      "\n",
      "        \u001b[37m# Model Output\u001b[39;49;00m\n",
      "        metrics_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        plt.savefig(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path))\n",
      "\n",
      "        report_dict = {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\n",
      "                },\n",
      "            },\n",
      "        }\n",
      "\n",
      "        evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            f.write(json.dumps(report_dict))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"tf_bert_reviews.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    volume_size=train_volume_size,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=\"2.3.1\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"validation_batch_size\": validation_batch_size,\n",
    "        \"test_batch_size\": test_batch_size,\n",
    "        \"train_steps_per_epoch\": train_steps_per_epoch,\n",
    "        \"validation_steps\": validation_steps,\n",
    "        \"test_steps\": test_steps,\n",
    "        \"use_xla\": use_xla,\n",
    "        \"use_amp\": use_amp,\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"freeze_bert_layer\": freeze_bert_layer,\n",
    "        \"enable_sagemaker_debugger\": enable_sagemaker_debugger,\n",
    "        \"enable_checkpointing\": enable_checkpointing,\n",
    "        \"enable_tensorboard\": enable_tensorboard,\n",
    "        \"run_validation\": run_validation,\n",
    "        \"run_test\": run_test,\n",
    "        \"run_sample_predictions\": run_sample_predictions,\n",
    "    },\n",
    "    input_mode=input_mode,\n",
    "    metric_definitions=metrics_definitions,\n",
    "    rules=rules,\n",
    "    debugger_hook_config=hook_config,\n",
    "    profiler_config=profiler_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Experiment Config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\"ExperimentName\": experiment_name, \"TrialName\": trial_name, \"TrialComponentDisplayName\": \"train\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\"train\": s3_input_train_data, \"validation\": s3_input_validation_data, \"test\": s3_input_test_data},\n",
    "    experiment_config=experiment_config,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  tensorflow-training-2021-04-12-21-49-31-083\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print(\"Training Job Name:  {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2021-04-12-21-49-31-083\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(\n",
    "            region, training_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2021-04-12-21-49-31-083;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\n",
    "            region, training_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-117859797117/tensorflow-training-2021-04-12-21-49-31-083/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(\n",
    "            bucket, training_job_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-117859797117/checkpoints/fab42251-467c-46a9-bfc6-8fa60a8276b1/?region=us-east-1&tab=overview\">S3 Checkpoint Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Checkpoint Data</a> After The Training Job Has Completed</b>'.format(\n",
    "            bucket, checkpoint_s3_prefix, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-04-12 21:49:31 Starting - Starting the training job\n",
      "2021-04-12 21:49:34 Starting - Launching requested ML instances..................\n",
      "2021-04-12 21:51:09 Starting - Preparing the instances for training..........\n",
      "2021-04-12 21:52:07 Downloading - Downloading input data.\n",
      "2021-04-12 21:52:16 Training - Downloading the training image...\n",
      "2021-04-12 21:52:38 Training - Training image download completed. Training in progress..........................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "2021-04-12 22:28:18 Uploading - Uploading generated training model.....................\n",
      "2021-04-12 22:30:07 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tensorflow-training-2021-04-12-21-49-31-083: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/local/bin/python3.7 tf_bert_reviews.py --enable_checkpointing False --enable_sagemaker_debugger True --enable_tensorboard True --epochs 3 --epsilon 1e-08 --freeze_bert_layer False --learning_rate 1e-05 --max_seq_length 64 --model_dir s3://sagemaker-us-east-1-117859797117/tensorflow-training-2021-04-12-21-49-31-083/model --run_sample_predictions True --run_test True --run_validation True --test_batch_size 128 --test_steps 100 --train_batch_size 128 --train_steps_per_epoch 100 --use_amp True --use_xla True --validation_batch_size 128 --validation_steps 100\"\n2021-04-12 21:52:42.075286: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-04-12 21:52:42.102211: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nWARNING:tensorflow:From tf_bert_reviews.py:87: map",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3069\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_train_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m         )\n\u001b[0;32m-> 3071\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3220\u001b[0m                 ),\n\u001b[1;32m   3221\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m             )\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tensorflow-training-2021-04-12-21-49-31-083: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/local/bin/python3.7 tf_bert_reviews.py --enable_checkpointing False --enable_sagemaker_debugger True --enable_tensorboard True --epochs 3 --epsilon 1e-08 --freeze_bert_layer False --learning_rate 1e-05 --max_seq_length 64 --model_dir s3://sagemaker-us-east-1-117859797117/tensorflow-training-2021-04-12-21-49-31-083/model --run_sample_predictions True --run_test True --run_validation True --test_batch_size 128 --test_steps 100 --train_batch_size 128 --train_steps_per_epoch 100 --use_amp True --use_xla True --validation_batch_size 128 --validation_steps 100\"\n2021-04-12 21:52:42.075286: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n2021-04-12 21:52:42.102211: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nWARNING:tensorflow:From tf_bert_reviews.py:87: map"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimator.latest_training_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait Until the ^^ Training Job ^^ Completes Above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-117859797117/tensorflow-training-2021-04-12-21-49-31-083/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers/\n",
      "transformers/fine-tuned/\n",
      "transformers/fine-tuned/tf_model.h5\n",
      "transformers/fine-tuned/config.json\n",
      "tensorboard/\n",
      "tensorboard/validation/\n",
      "tensorboard/validation/events.out.tfevents.1618265901.ip-10-0-151-4.ec2.internal.89.29704.v2\n",
      "tensorboard/validation/events.out.tfevents.1618264839.ip-10-0-151-4.ec2.internal.89.25864.v2\n",
      "tensorboard/train/\n",
      "tensorboard/train/events.out.tfevents.1618265900.ip-10-0-151-4.ec2.internal.89.29684.v2\n",
      "tensorboard/train/plugins/\n",
      "tensorboard/train/plugins/profile/\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.overview_page.pb\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.memory_profile.json.gz\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.trace.json.gz\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.tensorflow_stats.pb\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.kernel_stats.pb\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.input_pipeline.pb\n",
      "tensorboard/train/plugins/profile/2021_04_12_21_54_26/ip-10-0-151-4.ec2.internal.xplane.pb\n",
      "tensorboard/train/events.out.tfevents.1618264430.ip-10-0-151-4.ec2.internal.89.5924.v2\n",
      "tensorboard/train/events.out.tfevents.1618264466.ip-10-0-151-4.ec2.internal.profile-empty\n",
      "code/\n",
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00001\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/assets/\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./model/\n",
    "!tar -xvzf ./model.tar.gz -C ./model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-12 22:30:46.933404: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-12 22:30:46.933451: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 64)\n",
      "        name: serving_default_input_ids:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 64)\n",
      "        name: serving_default_input_mask:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 5)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/1')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_mask')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/1')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_mask')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_mask')]\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/1')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/0'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='inputs/1')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_mask')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: list\n",
      "          Value: [TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_ids'), TensorSpec(shape=(None, 64), dtype=tf.int32, name='input_mask')]\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir ./model/tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Debugger Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RuleConfigurationName': 'ProfilerReport',\n",
       "  'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-east-1:117859797117:processing-job/tensorflow-training-2021-0-profilerreport-ff29bba7',\n",
       "  'RuleEvaluationStatus': 'InProgress',\n",
       "  'LastModifiedTime': datetime.datetime(2021, 4, 12, 22, 30, 25, 725000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "balance_dataset                                       -> True\n",
      "balanced_bias_data_jsonlines_s3_uri                   -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "balanced_bias_data_s3_uri                             -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "bias_data_s3_uri                                      -> 's3://sagemaker-us-east-1-117859797117/bias-detect\n",
      "experiment_name                                       -> 'Amazon-Customer-Reviews-BERT-Experiment-161824731\n",
      "feature_group_name                                    -> 'reviews-feature-group-1618247324'\n",
      "feature_store_offline_prefix                          -> 'reviews-feature-store-1618247324'\n",
      "ingest_create_athena_db_passed                        -> True\n",
      "ingest_create_athena_table_parquet_passed             -> True\n",
      "ingest_create_athena_table_tsv_passed                 -> True\n",
      "max_seq_length                                        -> 64\n",
      "model_ab_endpoint_name                                -> 'tensorflow-training-2021-04-05-11-23-57-968-abtes\n",
      "processed_metrics_s3_uri                              -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_test_data_s3_uri                            -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_train_data_s3_uri                           -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processed_validation_data_s3_uri                      -> 's3://sagemaker-us-east-1-117859797117/sagemaker-s\n",
      "processing_evaluation_metrics_job_name                -> 'sagemaker-scikit-learn-2021-04-05-13-42-30-315'\n",
      "raw_input_data_s3_uri                                 -> 's3://sagemaker-us-east-1-117859797117/amazon-revi\n",
      "s3_private_path_tsv                                   -> 's3://sagemaker-us-east-1-117859797117/amazon-revi\n",
      "s3_public_path_tsv                                    -> 's3://amazon-reviews-pds/tsv'\n",
      "setup_dependencies_passed                             -> True\n",
      "tensorflow_endpoint_arn                               -> 'arn:aws:sagemaker:us-east-1:117859797117:endpoint\n",
      "tensorflow_endpoint_name                              -> 'tensorflow-training-2021-04-05-11-23-57-968-tf-16\n",
      "tensorflow_model_name                                 -> 'tensorflow-training-2021-04-05-11-23-57-968-tf-16\n",
      "test_data_bias_s3_uri                                 -> 's3://sagemaker-us-east-1-117859797117/bias/test_d\n",
      "test_data_explainablity_s3_uri                        -> 's3://sagemaker-us-east-1-117859797117/bias/test_d\n",
      "test_split_percentage                                 -> 0.05\n",
      "train_split_percentage                                -> 0.9\n",
      "training_job_name                                     -> 'tensorflow-training-2021-04-12-21-49-31-083'\n",
      "trial_name                                            -> 'trial-1618247314'\n",
      "validation_split_percentage                           -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the Experiment Tracking Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>enable_checkpointing</th>\n",
       "      <th>enable_sagemaker_debugger</th>\n",
       "      <th>enable_tensorboard</th>\n",
       "      <th>...</th>\n",
       "      <th>code - MediaType</th>\n",
       "      <th>code - Value</th>\n",
       "      <th>raw-input-data - MediaType</th>\n",
       "      <th>raw-input-data - Value</th>\n",
       "      <th>bert-test - MediaType</th>\n",
       "      <th>bert-test - Value</th>\n",
       "      <th>bert-train - MediaType</th>\n",
       "      <th>bert-train - Value</th>\n",
       "      <th>bert-validation - MediaType</th>\n",
       "      <th>bert-validation - Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow-training-2021-04-12-21-49-31-083-aws-training-job</td>\n",
       "      <td>train</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:117859797117:training-job/tensorflow-training-2021-04-12-21-49-31-083</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.c5.9xlarge</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensorflow-training-2021-04-12-20-53-49-447-aws-training-job</td>\n",
       "      <td>train</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:117859797117:training-job/tensorflow-training-2021-04-12-20-53-49-447</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.c5.9xlarge</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>false</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job</td>\n",
       "      <td>prepare</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/input/code/preprocess-scikit-text-to-bert-feature-store.py</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TrialComponentName  \\\n",
       "0       tensorflow-training-2021-04-12-21-49-31-083-aws-training-job   \n",
       "1       tensorflow-training-2021-04-12-20-53-49-447-aws-training-job   \n",
       "2  sagemaker-scikit-learn-2021-04-12-17-09-10-985-aws-processing-job   \n",
       "\n",
       "  DisplayName  \\\n",
       "0       train   \n",
       "1       train   \n",
       "2     prepare   \n",
       "\n",
       "                                                                                                SourceArn  \\\n",
       "0       arn:aws:sagemaker:us-east-1:117859797117:training-job/tensorflow-training-2021-04-12-21-49-31-083   \n",
       "1       arn:aws:sagemaker:us-east-1:117859797117:training-job/tensorflow-training-2021-04-12-20-53-49-447   \n",
       "2  arn:aws:sagemaker:us-east-1:117859797117:processing-job/sagemaker-scikit-learn-2021-04-12-17-09-10-985   \n",
       "\n",
       "                                                                SageMaker.ImageUri  \\\n",
       "0  763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37   \n",
       "1  763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37   \n",
       "2                                                                              NaN   \n",
       "\n",
       "   SageMaker.InstanceCount SageMaker.InstanceType  SageMaker.VolumeSizeInGB  \\\n",
       "0                      1.0          ml.c5.9xlarge                    1024.0   \n",
       "1                      1.0          ml.c5.9xlarge                    1024.0   \n",
       "2                      2.0          ml.c5.2xlarge                      30.0   \n",
       "\n",
       "  enable_checkpointing enable_sagemaker_debugger enable_tensorboard  ...  \\\n",
       "0                false                      true               true  ...   \n",
       "1                false                      true               true  ...   \n",
       "2                  NaN                       NaN                NaN  ...   \n",
       "\n",
       "   code - MediaType  \\\n",
       "0               NaN   \n",
       "1               NaN   \n",
       "2               NaN   \n",
       "\n",
       "                                                                                                                                      code - Value  \\\n",
       "0                                                                                                                                              NaN   \n",
       "1                                                                                                                                              NaN   \n",
       "2  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/input/code/preprocess-scikit-text-to-bert-feature-store.py   \n",
       "\n",
       "  raw-input-data - MediaType  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "\n",
       "                                          raw-input-data - Value  \\\n",
       "0                                                            NaN   \n",
       "1                                                            NaN   \n",
       "2  s3://sagemaker-us-east-1-117859797117/amazon-reviews-pds/tsv/   \n",
       "\n",
       "   bert-test - MediaType  \\\n",
       "0                    NaN   \n",
       "1                    NaN   \n",
       "2                    NaN   \n",
       "\n",
       "                                                                                       bert-test - Value  \\\n",
       "0                                                                                                    NaN   \n",
       "1                                                                                                    NaN   \n",
       "2  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-test   \n",
       "\n",
       "  bert-train - MediaType  \\\n",
       "0                    NaN   \n",
       "1                    NaN   \n",
       "2                    NaN   \n",
       "\n",
       "                                                                                       bert-train - Value  \\\n",
       "0                                                                                                     NaN   \n",
       "1                                                                                                     NaN   \n",
       "2  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-train   \n",
       "\n",
       "  bert-validation - MediaType  \\\n",
       "0                         NaN   \n",
       "1                         NaN   \n",
       "2                         NaN   \n",
       "\n",
       "                                                                                       bert-validation - Value  \n",
       "0                                                                                                          NaN  \n",
       "1                                                                                                          NaN  \n",
       "2  s3://sagemaker-us-east-1-117859797117/sagemaker-scikit-learn-2021-04-12-17-09-10-985/output/bert-validation  \n",
       "\n",
       "[3 rows x 65 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment_name,\n",
    "    metric_names=[\"validation:accuracy\"],\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Descending\",\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...2021-04-12-17-09-10-985/output/bert-test</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...4-12-17-09-10-985/output/bert-validation</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...021-04-12-17-09-10-985/output/bert-train</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76310...s.com/tensorflow-training:2.3.1-cpu-py37</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...ts/fab42251-467c-46a9-bfc6-8fa60a8276b1/</td>\n",
       "      <td>Output</td>\n",
       "      <td>Checkpoint</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction        Type  \\\n",
       "0  s3://...2021-04-12-17-09-10-985/output/bert-test     Input     DataSet   \n",
       "1  s3://...4-12-17-09-10-985/output/bert-validation     Input     DataSet   \n",
       "2  s3://...021-04-12-17-09-10-985/output/bert-train     Input     DataSet   \n",
       "3  76310...s.com/tensorflow-training:2.3.1-cpu-py37     Input       Image   \n",
       "4  s3://...ts/fab42251-467c-46a9-bfc6-8fa60a8276b1/    Output  Checkpoint   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "lineage_table_viz = LineageTableVisualizer(sess)\n",
    "lineage_table_viz_df = lineage_table_viz.show(training_job_name=training_job_name)\n",
    "lineage_table_viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
